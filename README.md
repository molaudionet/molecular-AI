Molecular AI has transformed drug discovery and materials science through symbolic representations (SMILES, InChI, molecular formulas) and graph structures. 
Yet a third modality remains unexplored: audio. We introduce MolAudioNet, a systematic framework for encoding molecular structures as audio waveforms, 
creating the first large-scale molecular audio dataset of 50,284 compounds. Our multimodal architecture---combining text, 
graph, and audio encoders---achieves state-of-the-art results: \textbf{95.2\% accuracy} on drug classification (15\% improvement over text-only), and \textbf{$R^2$=0.89} on property regression (23\% improvement).
Ablation studies reveal audio uniquely captures stereochemical and conformational information missed by other modalities. This work represents a step toward \textit{molecular foundation models} 
that understand chemistry through comprehensive multimodal representations, analogous to how large language models transformed NLP. We release MolAudioNet-50K, code, and pre-trained models to accelerate 
research in multimodal molecular intelligence.


https://osf.io/preprints/biohackrxiv/m4edg_v1

Zhou, C., & Zhou, E. (2026). MolAudioNet: Towards Molecular Foundation Models Through Multimodal Learning. Zenodo. https://doi.org/10.5281/zenodo.18120523


ORCID:
0009-0000-0630-3772

Transforming MRI k-Space Into Sound: A Novel Framework for Real-Time Quality Control Using Sonification and AI
Creators
Zhou, Jianping
 
Zhou, Emily (Contact person)
Cite all versions? You can cite all versions by using the DOI 10.5281/zenodo.17239696. This DOI represents all versions, and will always resolve to the latest one. Read mo


Publication  Open
Transforming MRI k-Space Into Sound: A Novel Framework for Real-Time Quality Control Using Sonification and AI
Creators
Zhou, Jianping
 
Zhou, Emily (Contact person)
Contributors
Contact person: 
Zhou, Emily
Description
Magnetic Resonance Imaging (MRI) quality control (QC) is essential

for ensuring diagnostic accuracy and reliability. Current QC

methods rely heavily on image-space evaluation or specialized phantoms,

which may miss subtle acquisition errors and require post hoc

analysis. We propose a novel framework that transforms raw MRI

k-space data into audio signals (sonification) and leverages artificial

intelligence (AI) audio feature extraction techniques to detect imaging

artifacts in real time. By converting k-space trajectories into audible

waveforms, anomalies such as motion, ghosting, RF spikes, or gradient

instabilities manifest as distinct acoustic patterns. Using audio features

such as Mel-Frequency Cepstral Coefficients (MFCCs), spectral

entropy, and spectrogram representations, our system trains machine

learning models to automatically identify acquisition errors. This approach

complements existing image-based QC methods and offers the

potential for a lightweight, audible, and real-time solution.
